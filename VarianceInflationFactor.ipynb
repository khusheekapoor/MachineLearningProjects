{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 - Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Khushee Kapoor\n",
    "\n",
    "Last Updated: 22/4/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we have imported the following libraries:\n",
    "\n",
    "- NumPy: to work with the data\n",
    "- Pandas: to manipulate the dataframe\n",
    "- MatPlotLib: for data visualization\n",
    "- Seaborn: for data visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn  as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read the dataset and store it into a dataframe using the read_csv() function from the Pandas library. We also create another dataset for comparsion purposes later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the dataset\n",
    "df = pd.read_csv('weatherAUS.csv')\n",
    "vifdf = pd.read_csv('weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we view the first few rows of the dataframe to get a glimpse of it. To do this, we use the head() function from the Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
       "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
       "0           W           44.0          W  ...        71.0         22.0   \n",
       "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
       "2         WSW           46.0          W  ...        38.0         30.0   \n",
       "3          NE           24.0         SE  ...        45.0         16.0   \n",
       "4           W           41.0        ENE  ...        82.0         33.0   \n",
       "\n",
       "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
       "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
       "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
       "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
       "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
       "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
       "\n",
       "   RainTomorrow  \n",
       "0            No  \n",
       "1            No  \n",
       "2            No  \n",
       "3            No  \n",
       "4            No  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Preprocess the data, remove the attributes which were are not useful to predict rain. Also, remove rows with at least one missing value for each of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sovle Question 1, we first view the dimensions of the dataframe by using the shape attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34978, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing the dimensions of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are 34978 rows and 23 columns in the dataset. Next, we check for missing values. To do that, we use the isnull() and sum() functions from the Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                 0\n",
       "Location             1\n",
       "MinTemp            500\n",
       "MaxTemp            370\n",
       "Rainfall           687\n",
       "Evaporation      19104\n",
       "Sunshine         23532\n",
       "WindGustDir       4936\n",
       "WindGustSpeed     4932\n",
       "WindDir9am        4472\n",
       "WindDir3pm        2101\n",
       "WindSpeed9am       831\n",
       "WindSpeed3pm      1475\n",
       "Humidity9am        666\n",
       "Humidity3pm       1330\n",
       "Pressure9am       6692\n",
       "Pressure3pm       6683\n",
       "Cloud9am         15814\n",
       "Cloud3pm         16140\n",
       "Temp9am            437\n",
       "Temp3pm           1105\n",
       "RainToday          687\n",
       "RainTomorrow       687\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is only one column - Date, with no missing values. To deal with the missing values, we follow the instructions and drop the rows using the dropna() function from the Pandas library. To maintain consistency between the duplicate dataset, we drop the values in that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the missing values\n",
    "df = df.dropna()\n",
    "vifdf = vifdf.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we encode the binary column RainToday with 1 for yes and 0 for no. To do this we use the map() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the column\n",
    "df['RainToday'] = df['RainToday'].map({'No':0, 'Yes':1})\n",
    "vifdf['RainToday'] = vifdf['RainToday'].map({'No':0, 'Yes':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we encode the columns with multiple categorical values. To do this, we loop over the categorical columns and create dummies using the get_dummies() funciton from the Pandas library. Then we merge the dummies using the merge() funcion and drop the original columns using the drop() function. To maintain consistency between the duplicate dataset, we carry out the same process in that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the categorical columns\n",
    "categorical_columns = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm']\n",
    "\n",
    "# encoding the values\n",
    "for column in categorical_columns:\n",
    "    \n",
    "    # in original dataframe\n",
    "    catdf = pd.get_dummies(df[column], prefix=column)    \n",
    "    df = pd.merge(left=df, right=catdf, left_index=True, right_index=True)    \n",
    "    df = df.drop(columns=column)\n",
    "    \n",
    "    # in duplicate dataframe\n",
    "    catdf = pd.get_dummies(vifdf[column], prefix=column)    \n",
    "    vifdf = pd.merge(left=vifdf, right=catdf, left_index=True, right_index=True)    \n",
    "    vifdf = vifdf.drop(columns=column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the original dataset into independent variables (x) and dependent variable (y) and use the train_test_split() function from the sklearn library and divide the dataset into training and testing sets. We also drop the Date column since it is not necessary for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into independent and dependent variables\n",
    "x = df.drop(columns=['Date', 'RainTomorrow'])\n",
    "y = df['RainTomorrow']\n",
    "\n",
    "# diving the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the LogisticRegression module from the sklearn library and build a classification model and train it on the original training set. Then we use the original testing set to calculate the accuracy and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Khushee\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# building the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# training the model\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "# printing the accuracy\n",
    "print(str.format('Accuracy: {:.2f}%', logreg.score(x_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the accuracy of the model is 83.71%, so the model performs moderately well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Calculate the Variance Inflation Factor (VIF) value. VIF is a number that determines whether a variable has multicollinearity or not (starts from 1, and it has no upper limit. If the number gets larger, it means the variable has huge multicollinearity on it.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve Question 2, we use the variance_inflation_factor() function from the statsmodels library on the numerical columns to obtain the VIF of each column. **We use the duplicate dataframe here onwards.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature            VIF\n",
      "0         MinTemp      79.232526\n",
      "1         MaxTemp     658.913695\n",
      "2        Rainfall       1.744358\n",
      "3     Evaporation       5.025893\n",
      "4        Sunshine      19.252299\n",
      "5   WindGustSpeed      29.240446\n",
      "6    WindSpeed9am       9.415210\n",
      "7    WindSpeed3pm      14.866406\n",
      "8     Humidity9am      76.366273\n",
      "9     Humidity3pm      64.338024\n",
      "10    Pressure9am  582307.075465\n",
      "11    Pressure3pm  579165.727765\n",
      "12       Cloud9am       8.386911\n",
      "13       Cloud3pm       9.098550\n",
      "14        Temp9am     300.273019\n",
      "15        Temp3pm     767.945037\n",
      "16      RainToday       2.292319\n"
     ]
    }
   ],
   "source": [
    "# importing the variance_inflation_factor() function\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# extracting the numerical columns\n",
    "cols = [cname for cname in vifdf.columns if vifdf[cname].dtype in ['int64', 'float64']]\n",
    "data = vifdf[cols]\n",
    "  \n",
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = data.columns\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data['VIF']= [variance_inflation_factor(data.values, i) for i in range(len(data.columns))]\n",
    "\n",
    "# printing the VIF of each feature\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the columns - MinTemp, MaxTemp, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, Temp9am, and Temp3pm have very high values of VIF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Remove multicollinearities by creating new features. Find the features that have paired values and create the new feature which is the difference value between those pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve Question 3, we engineer the following features:\n",
    "\n",
    "- Range: MaxTemp - MinTemp\n",
    "- WindSpeed: WindSpeed3pm - WindSpeed9am\n",
    "- Humidity: Humidity3pm - Humidity9am\n",
    "- Pressure: Pressure3pm - Pressure9am\n",
    "- Cloud: Cloud3pm - Cloud9am\n",
    "- Temp: Temp3pm - Temp9am\n",
    "\n",
    "Then, we drop the columns used to engineer the new features to remove the multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineering the new features\n",
    "vifdf['Range'] = vifdf['MaxTemp'] - vifdf['MinTemp']\n",
    "vifdf['WindSpeed'] = vifdf['WindSpeed3pm'] - vifdf['WindSpeed9am']\n",
    "vifdf['Humidity'] = vifdf['Humidity3pm'] - vifdf['Humidity9am']\n",
    "vifdf['Pressure'] = vifdf['Pressure3pm'] - vifdf['Pressure9am']\n",
    "vifdf['Cloud'] = vifdf['Cloud3pm'] - vifdf['Cloud9am']\n",
    "vifdf['Temp'] = vifdf['Temp3pm'] - vifdf['Temp9am']\n",
    "\n",
    "# dropping the original columns\n",
    "vifdf = vifdf.drop(columns=['MaxTemp', 'MinTemp', 'WindSpeed3pm', 'WindSpeed9am', 'Humidity3pm', 'Humidity9am', 'Pressure3pm',\n",
    "                     'Pressure9am', 'Cloud3pm', 'Cloud9am', 'Temp3pm', 'Temp9am'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the variance_inflation_factor() function from the statsmodels library on the numerical columns to obtain the VIF of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature        VIF\n",
      "0        Rainfall   1.705682\n",
      "1     Evaporation   3.567996\n",
      "2        Sunshine   7.267535\n",
      "3   WindGustSpeed   7.052104\n",
      "4       RainToday   2.145453\n",
      "5           Range  17.785578\n",
      "6       WindSpeed   1.272272\n",
      "7        Humidity   5.179944\n",
      "8        Pressure   4.353885\n",
      "9           Cloud   1.139004\n",
      "10           Temp  12.927773\n"
     ]
    }
   ],
   "source": [
    "# importing the variance_inflation_factor() function\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# extracting the numerical columns\n",
    "cols = [cname for cname in vifdf.columns if vifdf[cname].dtype in ['int64', 'float64']]\n",
    "data = vifdf[cols]\n",
    "  \n",
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = data.columns\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data['VIF']= [variance_inflation_factor(data.values, i) for i in range(len(data.columns))]\n",
    "\n",
    "# printing the VIF of each feature\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have significantly reduced the VIF for many columns by engineering new features and removing the original ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Remove features that have a VIF value above 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve Question 4, we drop the columns Sunshine, WindGustSpeed, Range, and Temp since they have VIF above 5. Next we split the duplicate dataset into independent variables (x) and dependent variable (y) and use the train_test_split() function from the sklearn library and divide the dataset into training and testing sets. We also drop the Date column since it is not necessary for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into independent and dependent variables\n",
    "x = vifdf.drop(columns=['Date', 'Sunshine', 'WindGustSpeed', 'Range', 'Temp', 'RainTomorrow'])\n",
    "y = vifdf['RainTomorrow']\n",
    "\n",
    "# diving the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Build a regression model to perform the Rain prediction. Also, tabulate accuracy of the prediction models, before and VIF computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve Question 5, we import the LogisticRegression module from the sklearn library and build a classification model and train it on the duplicate training set. Then we use the duplicate testing set to calculate the accuracy and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Khushee\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# building the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# training the model\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "# printing the accuracy\n",
    "print(str.format('Accuracy: {:.2f}%', logreg.score(x_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the accuracy of the model is 79.39%. The accuracy has slightly dropped in comparison to the model built using the original dataset. However, the model is now much more stable because we have removed the multicollinearity, so the variance of the coefficient estimate is stable and is not very sensitive to minor changes in the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
